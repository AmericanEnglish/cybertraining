Team 6 Project of the CyberTraining program at UMBC in 2020 (http://cybertraining.umbc.edu/)

**Title**: Studying Arctic Sea Ice Retreat Using Data-Driven Causality Discovery Approaches

**Team members**: Yiyi Huang, Matthäus Kleindessner, Alexey Munishkin, Debvrat Varshney

**Mentors**: Jianwu Wang, Pei Guo

**Abstract**: The Arctic sea ice has retreated rapidly in the past few decades, which is believed to be driven by various dynamic and thermodynamic processes in the atmosphere. The newly open water resulted from sea ice decline in turn exerts large influence on the atmosphere. Therefore, this study aims to investigate the causality between multiple atmospheric processes and sea ice variations using three distinct data-driven causality approaches: TCDF, NOTEARS and DAG-GNN. We find that the static graphs produced by NOTEARS and DAG-GNN are relatively reasonable. The results from NOTEARS indicate that relative humidity and precipitation dominate sea ice changes among all variables, while the results from DAG-GNN suggest that the horizontal wind fields are more important for driving sea ice variations. However, both of them produce some unrealistic edges. In comparison, the temporal graphs generated by the three methods are not physically meaningful enough. It also turns out that the results are rather sensitive to the choice of hyperparameters of the three methods. As a pioneer study, this work paves the way for us to disentangle the complex causal relationships in the Earth system, by taking the advantage of cutting-edge Artificial Intelligence technologies.

**Instructions on how to run the code**:

For plotting TCDF, NOTEARS and DAG-GNN graphs, you need the [igraph](https://igraph.org/python/) package.

###### Preprocessing

To download the ERA-5 global reanalysis dataset, you need to install the Climate Data Store (CDS) API on your machine. And then you can run the following code by specifying the variable names, temporal and spatial resolution. This code is adapted and modified from European Centre for Medium-Range Weather Forecasts (ECMWF) website. Additional details can be found from https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5.

download_ERA5.py

The data downloaded from ECMWF is saved in netCDF format. Therefore, you can run the following code, which reads the data from the netCDF file, checks missing values, and averages the data points in the Arctic (60˚N northward) and saves time series into CSV file.

average_ERA5_Arctic_data.py

The following code is used to decompose time series into trend, seasonality and residual terms. And only residual terms are saved into CSV file.

decompose_time_series_data.py

Then you can combine multiple CSV files for different variables into one CSV file by running

combine_decomposed_data.py

The output for this step is a combined dataset with residual terms for all variables.

*/Data/combined_decomposed.csv

The following code is used to drop missing values and normalize the data using max-min method.

normalize_data.py

You will find the output for this step as below. This dataset is the input for TCDF and static model for NOTEARS and DAG-GNN.

*/Data/combined_decomposed_norm_1980_2018.csv

Then you can read this dataset and get lagged data for temporal model (only for NOTEARS and DAG-GNN) by running

get_lagged_data.py

You will get the output as below.

*/Data/lagged_combined_decomposed_norm_1980_2018.csv

###### TCDF

In order to run the code, you need to download the TCDF code from [this link](https://github.com/M-Nauta/TCDF). The instructions to run the code is given on that link, but below are specific instructions for this project.

We run the code on [taki high performance computing faculty](https://hpcf.umbc.edu/) so first you need to
```
module load networkx
module --ignore-cache load "Python/3.7.6-intel-2019a"
```
then you can run the TCDF code
```
python runTCDF.py --data report_data/combined_decomposed_drop_temp_all_norm_1980_2018.csv --kernel_size k# --hidden_layers h#
```
where `report_data/combined_...` is the same csv data used for the NOTEARS and DAG_GNN code. Note: `k#` and `h#` are used for the hyperparameter sensitivity study where for this project we used `k# = {2,4,6}` and `h# = {0,1,2}`. Note: replace `k#` and `h#` with numeric integer values.

After you run the TCDF code you will get a text display, e.g.
```
python runTCDF.py --data report_data/combined_decomposed_drop_temp_all_norm_1980_2018.csv --kernel_size 2 --hidden_layers 0
...
...
===================Results for combined_decomposed_drop_temp_all_norm_1980_2018.csv ==================================
v10m causes u10m with a delay of 0 time steps.
u10m causes v10m with a delay of 0 time steps.
========================================================================
```
which can (manually) converted to a numpy adjacency matrix
```
([[0,0,0,0,0,0,0,0,0,0,0,0],
 [0,0,0,0,0,0,0,0,0,0,0,0],
 [0,0,0,0,0,0,0,0,0,0,0,0],
 [0,0,0,0,0,0,0,0,0,0,0,0],
 [0,0,0,0,0,0,0,0,0,0,0,0],
 [0,0,0,0,0,0,0,0,0,0,0,0],
 [0,0,0,0,0,0,0,1,0,0,0,0],
 [0,0,0,0,0,0,1,0,0,0,0,0],
 [0,0,0,0,0,0,0,0,0,0,0,0],
 [0,0,0,0,0,0,0,0,0,0,0,0],
 [0,0,0,0,0,0,0,0,0,0,0,0],
 [0,0,0,0,0,0,0,0,0,0,0,0]])
```
where the going from left to right or up to down is `['HFLX','SW','LW','SLP','Precip','RH','u10m','v10m','sea_ice','CC','CW','GH']`, which are aliases for atmospheric and sea ice variables (this is also used by NOTEARS and DAG-GNN)

* longwave: LW
* shortwave: SW
* tot_precip: Precip
* u10m: u10m
* v10m: v10m
* sea_ice: sea_ice
* GH_mean: GH
* SLP: SLP
* RH: RH
* cloud_cover: CC
* cloud_water: CW
* heat_flux: HFLX

now we can plot the data (though you have to manually adjust the numpy matrices in the code)
```
python plot_graph_for_TCDF.py
```
and for the hyperparameter sensitivity study (make sure to manually adjust the matrices here too)
```
python compute_normHamming_for_TCDF.py
```

###### NOTEARS

In order to run the code, you need to download the NOTEARS code from https://github.com/xunzheng/notears. We used Version 2.1 of the NOTEARS code.

Then just run 
```
run_notears_on_data_STATIC.py 
```
or 
```
run_notears_on_data_TEMPORAL.py 
```

###### DAG-GNN

Copy the `src` folder from [this link](https://github.com/big-data-lab-umbc/DAG-GNN) and place it inside the DAG-GNN folder here. Then put the detrended and deaseasonalized datasets inside the `data` folder in csv format. Your DAG-GNN folder structure should look like this -
* DAG-GNN
  * data
    * file1.csv
    * file2.csv
    * .
    * .
  * plots
    * plot_graph.py
    * plotGNN.py
  * src
    * train.py
    * utils.py
    * modules.py

To run DAG-GNN, just run the following command from inside the `src` folder
```
python train.py --filename=file1.csv --epochs=50 
```
Your DAG will be generated at this location: `src/<filename>__epochs<no. of epochs>/predG`. For example, for the command above, your DAG will be a file called "predG" generated in `src/file1__epochs50/`.

Further, DAG-GNN can be customized with a whole list of arguments, but the `filename` argument is compulsory. If the number of epochs is not defined, then its default value will be considered, which is 200.

To plot the graphs, simply run the following command from inside the `plots` folder
```
python plotGNN.py 
```
Each graph will be generated in the folder containing the corresponding "predG".

###### Postprocessing

plot_graph.py is used to generate causality graph in PDF based on the adjacency matrix of discovered causal relationships in CSV
